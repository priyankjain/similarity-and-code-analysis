diff --git a/home/priyank/Desktop/Dropbox/RCODI/similarity-and-code-analysis/2016greenironhack-phase2/2016greenironhack-lheston/Documentation.txt b/home/priyank/Desktop/Dropbox/RCODI/similarity-and-code-analysis/2016greenironhack-phase3/2016greenironhack-lheston/Documentation.txt
index 0566a4e..6fb5eda 100644
--- a/home/priyank/Desktop/Dropbox/RCODI/similarity-and-code-analysis/2016greenironhack-phase2/2016greenironhack-lheston/Documentation.txt
+++ b/home/priyank/Desktop/Dropbox/RCODI/similarity-and-code-analysis/2016greenironhack-phase3/2016greenironhack-lheston/Documentation.txt
@@ -1,3 +1,8 @@
 Doc:
 
 Currently developed an initial tableau viz using the food almanac data set from Data.Gov. I am also currently on working on implementing the weather data set but do not plan on using it as much as other data sources. I am also developing a tool to crawl the twitterAPI in order to search every zipcode in indiana for specific results in order to determine where people are talking about fresh farm grown vegtables. I am starting the crawler later tonight and saving the data in a sqlite db, I was wondering if there was an instance I could also use for this project? The schema of the table is included as well.
+
+
+UPDATE PHASE 3:
+
+I have finished my data puller from twitter, it has completed one complete pull and has started its second to pull twitter data for two days. After this I will input into tableau to create the final viz. I got the OK at the very begining of the project ot use Tableau as long as it works web based so I am not sure why I am recieving low marks for using it. example data should be included in this submission as well.....
diff --git a/home/priyank/Desktop/Dropbox/RCODI/similarity-and-code-analysis/2016greenironhack-phase3/2016greenironhack-lheston/tweety.py b/home/priyank/Desktop/Dropbox/RCODI/similarity-and-code-analysis/2016greenironhack-phase3/2016greenironhack-lheston/tweety.py
new file mode 100644
index 0000000..a1565d1
--- /dev/null
+++ b/home/priyank/Desktop/Dropbox/RCODI/similarity-and-code-analysis/2016greenironhack-phase3/2016greenironhack-lheston/tweety.py
@@ -0,0 +1,128 @@
+import tweepy as tp
+import sqlite3
+import pandas as pd 
+import numpy as np
+import time as t
+from geopy.geocoders import Nominatim
+import string
+from numpy import genfromtxt
+from geopy.exc import GeocoderTimedOut
+###radius use 15 miles
+###use nohup in bash to continuously run
+
+###geo stuff
+
+
+##tquery
+query = 'vegtables OR farmers market OR cheap food'
+radius = 500
+count = 200
+DataSet = pd.DataFrame
+loopCount = 0
+apCount = 0
+dn = []
+
+##read in zipcode file
+#zips = pd.read_csv("zips", delim_whitespace=True, header=None, names = ['a'])
+
+
+#zips = genfromtxt("zips")
+zips = genfromtxt("zips")
+
+conn=sqlite3.connect('twit.db')
+c = conn.cursor()
+
+
+#40.665137,-84.919308
+
+
+def location(zip):
+    geolocator = Nominatim()
+    t.sleep(5)
+    location = do_geocode(zip)
+    cordinates = ((location.latitude,location.longitude))
+    cordinates = str(cordinates)
+    cordinates = cordinates.replace("(","")
+    cordinates = cordinates.replace(")","")
+    cordinates = cordinates.replace(" ", "")
+    return cordinates
+
+def toDataFrame(tweets,zip):
+
+    DataSet = pd.DataFrame()
+
+    
+    DataSet['tweetText'] = [tweet.text for tweet in tweets]
+    DataSet['tweetID'] = [tweet.id for tweet in tweets]
+    DataSet['tweetRetweetCt'] = [tweet.retweet_count for tweet in tweets]
+    DataSet['tweetFavoriteCt'] = [tweet.favorite_count for tweet in tweets]
+    DataSet['tweetSource'] = [tweet.source for tweet in tweets]
+    DataSet['tweetCreated'] = [tweet.created_at for tweet in tweets]
+    DataSet['userID'] = [tweet.user.id for tweet in tweets]
+    DataSet['userScreen'] = [tweet.user.screen_name for tweet in tweets]
+    DataSet['userName'] = [tweet.user.name for tweet in tweets]
+    DataSet['userCreateDt'] = [tweet.user.created_at for tweet in tweets]
+    DataSet['userDesc'] = [tweet.user.description for tweet in tweets]
+    DataSet['userFollowerCt'] = [tweet.user.followers_count for tweet in tweets]
+    DataSet['userFriendsCt'] = [tweet.user.friends_count for tweet in tweets]
+    DataSet['userLocation'] = [tweet.user.location for tweet in tweets]
+    DataSet['userTimezone'] = [tweet.user.time_zone for tweet in tweets]
+    DataSet['zipcodeQuery'] = [zip for tweet in tweets]
+    #DataSet['lat'] = [tweet.geo.lat for tweet in tweets]
+
+    return DataSet
+
+def curTweepy(inputCode, count, query):
+    for items in tp.Cursor(api.search(geocode=inputCode, count=count, q=query)).items():
+        return items
+
+
+def do_geocode(address):
+    geolocator = Nominatim()
+    try:
+        return geolocator.geocode(address, timeout=None)
+    except GeocoderTimedOut:
+        return do_geocode(address)
+
+def lookUp(results,zip):
+    for result in results:
+        DataSet = toDataFrame(results,zip)
+        return DataSet
+
+auth = tp.OAuthHandler('7kXWkyNwmrDvwyy6THDxsRRS6','jUPVaJuNUyW2GKT3x1rwabwQJjpx2Tsr4NiiiDaUbSLTeZy8x8')
+auth.set_access_token('79074669-tlMikTK1RzGQukzlFBbV0kcVXJdMbPmUk4gPCsqVk', 'bQ4xHOXEd1o9ScDmn029nqemS8ZJSZU3ToO1aVyka2tjR')
+
+api = tp.API(auth)
+
+for zip in zips:
+#for row, zip in zips.iterrows():
+    if (loopCount == 15):
+        t.sleep(960)
+        loopCount = 0
+    loopCount = loopCount + 1
+    cordinates = location(zip)
+    inputCode = str(cordinates) + ',' + str(radius) + 'mi'
+    #inputCode = '{0},{1}mi'.format(cordinates, radius)
+    #inputCode = '41.1255471,-85.1949746,400mi'
+    results = api.search(geocode=inputCode, count=100, q=query)
+    #results = curTweepy(inputCode, count, query)
+    #print('results retrieved')
+    #print results
+    DataSet = pd.DataFrame()
+    DataSet = lookUp(results,zip)
+    if DataSet is not None:
+        DataSet.to_sql(con=conn, name='data', flavor='sqlite', if_exists='append')
+    dn.append(DataSet)
+    print loopCount
+
+
+dn = pd.concat(dn, axis=1)
+print dn
+#dn.to_sql(con=conn, name='data', flavor='sqlite', if_exists='append')
+
+
+
+
+
+
+
